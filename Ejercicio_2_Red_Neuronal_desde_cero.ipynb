{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM1BHFa5jVFndYKH8D49cIU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EstebanmAcero/Neural_nets/blob/main/Ejercicio_2_Red_Neuronal_desde_cero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Luis Esteban Molina Acero 201910203**\n",
        "#**Manuel Velez PhD.**\n"
      ],
      "metadata": {
        "id": "28Vx8VtJvTUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Crear el dataset\n",
        "n = 500\n",
        "p = 2\n",
        "X, Y = make_circles(n_samples=n, factor=0.5, noise=0.03)\n",
        "\n",
        "# Graficar los datos\n",
        "plt.scatter(X[Y == 0, 0], X[Y == 0, 1], c=\"skyblue\")\n",
        "plt.scatter(X[Y == 1, 0], X[Y == 1, 1], c=\"salmon\")\n",
        "plt.axis(\"equal\")\n",
        "plt.show()\n",
        "\n",
        "# Definición de la clase para una capa de la red neuronal\n",
        "class neural_layer():\n",
        "    def __init__(self, n_conn, n_neur, act_f):\n",
        "        self.act_f = act_f\n",
        "        self.b = np.random.rand(1, n_neur) * 2 - 1  # Inicialización aleatoria de los sesgos\n",
        "        self.W = np.random.rand(n_conn, n_neur) * 2 - 1  # Inicialización aleatoria de los pesos\n",
        "\n",
        "# Definición de funciones de activación (Sigmoid y su derivada)\n",
        "sigm = (lambda x: 1 / (1 + np.exp(-x)), lambda x: x * (1 - x))\n",
        "\n",
        "# Crear una capa de la red neuronal\n",
        "_l0 = neural_layer(p, 4, sigm)\n",
        "_l1 = neural_layer(4, 8, sigm)\n",
        "\n",
        "# Función para crear una red neuronal con la topología especificada\n",
        "def create_nn(topology, act_f):\n",
        "    nn = []\n",
        "    for l, layer in enumerate(topology[:-1]):\n",
        "        nn.append(neural_layer(topology[l], topology[l + 1], act_f))\n",
        "    return nn\n",
        "\n",
        "# Definir la topología de la red y crear la red\n",
        "topology = [p, 4, 8, 16, 32, 8, 4, 1]\n",
        "neural_net = create_nn(topology, sigm)\n",
        "\n",
        "# Función de costo (Error cuadrático medio)\n",
        "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr) ** 2), lambda Yp, Yr: (Yp - Yr))\n",
        "\n",
        "def train(neural_net, X, Y, l2_cost, lr=0.5):\n",
        "    out = [(None, X)]\n",
        "\n",
        "    # Forward pass (propagación hacia adelante)\n",
        "    for l, layer in enumerate(neural_net):\n",
        "        z = out[-1][1] @ neural_net[l].W + neural_net[l].b  # Combinación lineal de entradas y pesos\n",
        "        a = neural_net[l].act_f[0](z)  # Aplicación de la función de activación\n",
        "        out.append((z, a))  # Guardar las salidas intermedias\n",
        "\n",
        "    print(\"Costo inicial:\", l2_cost[0](out[-1][1], Y))\n",
        "\n",
        "    if train:\n",
        "        deltas = []\n",
        "\n",
        "        # Backward pass (propagación hacia atrás)\n",
        "        for l in reversed(range(0, len(neural_net))):\n",
        "            z = out[l + 1][0]\n",
        "            a = out[l + 1][1]\n",
        "\n",
        "            if l == len(neural_net) - 1:\n",
        "                deltas.insert(0, l2_cost[1](a, Y) * neural_net[l].act_f[1](a))  # Calcular el error y la derivada\n",
        "            else:\n",
        "                deltas.insert(0, deltas[0] @ neural_net[l + 1].W.T * neural_net[l].act_f[1](a))\n",
        "\n",
        "            _w = neural_net[1].W\n",
        "\n",
        "            # Descenso del gradiente\n",
        "            neural_net[l].b = neural_net[l].b - np.mean(deltas[0], axis=0, keepdims=True) * lr\n",
        "            neural_net[l].W = neural_net[l].W - out[l][1].T @ deltas[0] * lr\n",
        "\n",
        "    print(\"Costo final:\", l2_cost[0](out[-1][1], Y))\n",
        "    return out[-1][1]\n",
        "\n",
        "# Entrenar la red neuronal\n",
        "train(neural_net, X, Y, l2_cost, 0.5)\n"
      ],
      "metadata": {
        "id": "KiRTt34w9P6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "neural_n = create_nn(topology, sigm)\n",
        "\n",
        "loss = []\n",
        "\n",
        "for i in range(1):\n",
        "    # Entrenamos a la red neuronal\n",
        "    pY = train(neural_n, X, Y, l2_cost)\n",
        "\n",
        "    if i % 25 == 0:\n",
        "        loss.append(l2_cost[0](pY, Y))\n",
        "\n",
        "        res = 50\n",
        "        _x0 = np.linspace(-1.5, 1.5, res)\n",
        "        _x1 = np.linspace(-1.5, 1.5, res)\n",
        "        Y_ = np.zeros((res, res))\n",
        "\n",
        "        for i0, x0 in enumerate(_x0):\n",
        "            for i1, x1 in enumerate(_x1):\n",
        "                Y_[i0, i1] = train(neural_n, np.array([[x0, x1]]), Y, l2_cost)[0][0]\n",
        "\n",
        "        plt.pcolormesh(_x0, _x1, Y_, cmap=\"coolwarm\")\n",
        "        plt.axis(\"equal\")\n",
        "        plt.scatter(X[Y == 0, 0], X[Y == 0, 1], c=\"skyblue\")\n",
        "        plt.scatter(X[Y == 1, 0], X[Y == 1, 1], c=\"salmon\")\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        plt.plot(range(len(loss)), loss)\n",
        "        plt.show()\n",
        "        time.sleep(0.5)\n"
      ],
      "metadata": {
        "id": "QzERG2wRlfOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5cLYj2JF5_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22z_jxnlKiJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}