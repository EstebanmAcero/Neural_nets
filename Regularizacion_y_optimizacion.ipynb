{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBjLxM3mer5pEGh33cjCZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EstebanmAcero/Neural_nets/blob/main/Regularizacion_y_optimizacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Luis Esteban Molina Acero 201910203**\n",
        "\n",
        "### **Manuel Velez**"
      ],
      "metadata": {
        "id": "i198NglFfU76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularización"
      ],
      "metadata": {
        "id": "0ns7v4J4HmR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La regularización es una tecnica para evitar el overfitting en Machine Learning, esto se refiere qa que el modelo aprenda demasiado con los datos de entrenamiento que generalice respuestas para datos de entrada distintos.\n",
        "\n",
        "Este puede ser validado contrastando las curvas de entrenamiento y validación con respecto a la pérdida.\n",
        "\n",
        "Por ejemplo si la el error de validación incrementa mientras que el error de entrenamiento se disminuye se puede tener una situación de overfitting."
      ],
      "metadata": {
        "id": "y5xyFZdGGg0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se prepara el dataset usando el dataset del iris de la flor de Anderson: el cual contiene 150 registros bajo 5 atributos tales como :    \n",
        "\n",
        "1. Longitud del sepalo.\n",
        "2. Ancho del sepalo.\n",
        "3. Longitud del petalo.\n",
        "4. Ancho del petalo.\n",
        "5. Clase o objetivo .\n"
      ],
      "metadata": {
        "id": "3sRgCqoOITE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48AXUH36GJq2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a explorar los datos de manera que se carguen los datos en un DataFrame"
      ],
      "metadata": {
        "id": "na7G_BFOIxVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data into a dataframe\n",
        "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
        "\n",
        "# convert datatype to float\n",
        "df['label'] = iris.target\n",
        "\n",
        "# use string label instead\n",
        "df['label'] = df.label.replace(dict(enumerate (iris.target_names)))"
      ],
      "metadata": {
        "id": "Yz9Z-0QIICB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "N_C1qimNJRL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realiza la transformación con One Hot encoding para pasar de variables categoricas a numericas."
      ],
      "metadata": {
        "id": "zTVnTjKLKoGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# label -> one-hot encoding\n",
        "label = pd.get_dummies(df['label'], prefix = 'label')\n",
        "df = pd.concat([df, label], axis = 1)\n",
        "\n",
        "# drop old label\n",
        "df.drop(['label'], axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "5S2VHgwHJ-BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Kv9sa-cZKfyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos a crear variables x and y\n",
        "# Keras and TensorFlow solo aceptan arreglos como entradas\n",
        "# asi que vamos a convertir el dataframe en un arreglo\n",
        "\n",
        "X = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
        "\n",
        "# Convert dataframe into np array\n",
        "X = np.asarray(X)\n",
        "\n",
        "y = df[['label_setosa', 'label_versicolor','label_virginica']]\n",
        "\n",
        "# Convert DataFrame into np array\n",
        "y = np.asarray(y)"
      ],
      "metadata": {
        "id": "BGQmaQFhL4YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a separar los datos para el entrenamiento y la prueba usando\n",
        "\n",
        "\"train_test_spit() \"\n"
      ],
      "metadata": {
        "id": "3MLlkAEcNDGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n"
      ],
      "metadata": {
        "id": "EK4r0Sm6XSts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vamos a construir el model de red neuronal sin regularizacion"
      ],
      "metadata": {
        "id": "yEN3KVHfXofg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(4,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# la primer capa tiene un tamaño de 4, de 64 unidades seguida por tres capas\n",
        "# de 128 unidades además hay 3 capas de 64 unidades densas\n",
        "# todas con la función de activación RELU\n",
        "# la capa de salida tiene 3 unidades y una función softmax de activacíón\n"
      ],
      "metadata": {
        "id": "4o6qUYBeXnHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a correr el modelo\n",
        "\n",
        "model = create_model()\n",
        "model.summary()  # se crea un resumen del modelo"
      ],
      "metadata": {
        "id": "mwafko1TYeNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se procede a configurar el modelo con compile() pasando los siguientes argumentos:\n",
        "\n",
        "\n",
        "* adam : un algoritmo optimizador.\n",
        "* categorical_corssentropy : para un problema de clasificación múltiple.\n",
        "* acurracy : para tener métricas de evaluación para el entranamiento y prueba.\n"
      ],
      "metadata": {
        "id": "nAa7gZaUYogv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(  optimizer  ='adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
        "#model.compile( optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'acuracy')\n",
        "#model.compile( optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'acurracy',  run_eagerly=True)\n"
      ],
      "metadata": {
        "id": "kcos5JAjZKRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n"
      ],
      "metadata": {
        "id": "KRfbt4gXahtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilizamos model.fit() acomodar el modelo a los datos de entrenamiento\n",
        "history = model.fit(x_train, y_train, epochs=200, validation_split=0.25, batch_size=40, verbose=2)\n",
        "#history = model.fit( x_train, y_train, epochs = 200, validation_split = 0.25, batch_size = 40, verbose = 2)"
      ],
      "metadata": {
        "id": "y6vN3UEUZf4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se evalua el modelo una vez que se ha entrenado el modelo por lo que implica:\n",
        "\n",
        "1. Una gráfica de las métricas de pérdida y de acurracy.\n",
        "2. Probar el modelo contra datso de entrenamiento nunca usados.\n",
        "\n",
        "Por lo anterior se crea la funcion plot_metric().\n",
        "\n"
      ],
      "metadata": {
        "id": "n9nDEPWucSO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "def plot_metric(history, metric) :\n",
        "    train_metrics  = history.history [metric]\n",
        "    val_metrics    = history.history ['val_'+ metric]\n",
        "    epochs         = range (1, len(train_metrics) + 1)\n",
        "    plt.plot (epochs, train_metrics)\n",
        "    plt.plot (epochs, val_metrics)\n",
        "    plt.title('Training and valitation ' + metric)\n",
        "    plt.xlabel (\"Epochs\")\n",
        "    plt.ylabel (metric)\n",
        "    plt.legend([\"train_\"+metric, 'val_'+ metric])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_metric(history, 'categorical_accuracy')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SOvZb4sIcJ45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(history, 'loss')"
      ],
      "metadata": {
        "id": "XJ0PMH9Od0Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a evaluar el modelo con el set de prueba\n",
        "\n",
        "model.evaluate (x_test, y_test, verbose = 2)"
      ],
      "metadata": {
        "id": "_KAdegx6gkhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Añadiendo la regularización L2 y Dropout\n",
        "\n"
      ],
      "metadata": {
        "id": "CJH0x1KXgvlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers        import Dropout\n",
        "from tensorflow.keras.regularizers  import l2\n"
      ],
      "metadata": {
        "id": "4ofy3gA_g5Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear una funccion llamada _create_regularized_model_() la cual retorna un modelo similar al construido pero esta vez añadira Regularización L2 y capas Dropout, por lo que está función toma dos argumentos:\n",
        "\n",
        "1. El factor de regularización L2.\n",
        "2. La tasa de Dropout.\n",
        "\n"
      ],
      "metadata": {
        "id": "cjTq9GBChFku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_regularized_model(factor, rate):\n",
        "    model = Sequential([\n",
        "        Dense(64, kernel_regularizer=l2(factor), activation=\"relu\", input_shape=(4,)),\n",
        "        Dropout(rate),\n",
        "        Dense(128, kernel_regularizer=l2(factor), activation=\"relu\"),\n",
        "        Dropout(rate),\n",
        "        Dense(128, kernel_regularizer=l2(factor), activation=\"relu\"),\n",
        "        Dropout(rate),\n",
        "        Dense(128, kernel_regularizer=l2(factor), activation=\"relu\"),\n",
        "        Dropout(rate),\n",
        "        Dense(64, kernel_regularizer=l2(factor), activation=\"relu\"),\n",
        "        Dropout(rate),\n",
        "        Dense(64, kernel_regularizer=l2(factor), activation=\"relu\"),\n",
        "        Dropout(rate),\n",
        "        Dense(64, kernel_regularizer=l2(factor), activation=\"relu\"),\n",
        "        Dropout(rate),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "_McvTWuLh5gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear un model con argumentos L2 con el factor 0.0001 y un Dropout rate de 0.3\n"
      ],
      "metadata": {
        "id": "LHVD8hiUh8mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_regularized_model (1e-5, 0.3)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jwcHtNZHiFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo regularizado puede ser entrenado tal como construimos el primer modelo.\n"
      ],
      "metadata": {
        "id": "rejpS0bdiLu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar el modelo usando la función model.compile()\n",
        "\n",
        "model.compile( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['categorical_accuracy'])\n",
        "\n",
        "# Ahora, entrenamos el modelo con fit()\n",
        "\n",
        "history = model.fit( x_train, y_train, epochs = 200, validation_split = 0.25, batch_size = 40, verbose = 2)\n"
      ],
      "metadata": {
        "id": "uG4jBaA6iWJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Volvemos a evaluar el modelo\n",
        "\n",
        "plot_metric(history, 'loss')"
      ],
      "metadata": {
        "id": "WPJWaxZUjN6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si bien el overfitting no esta completamente arreglado, hay una mejora significativa comparada con el modelo sin regularización.\n",
        "\n",
        "Por lo que volvemos a evaluar el modelo.\n"
      ],
      "metadata": {
        "id": "4og2MikEjTcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test, verbose = 2)"
      ],
      "metadata": {
        "id": "vf2Ms4bhjlJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimización"
      ],
      "metadata": {
        "id": "bkmV52_Qj6-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons\n"
      ],
      "metadata": {
        "id": "1jc7ci3XNyZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow tensorflow-addons adabelief-tf\n"
      ],
      "metadata": {
        "id": "j2NK34NLOS5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow_addons.optimizers import RectifiedAdam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar y preprocesar el conjunto de datos MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalizar los valores de píxeles a [0, 1]\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Configuración de hiperparámetros\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# Construir el modelo de la red neuronal\n",
        "model = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),  # Aplanar la imagen\n",
        "    layers.Dense(128),  # Capa completamente conectada sin activación aquí\n",
        "    layers.Activation('relu'),  # Añadir la activación como una capa separada\n",
        "    layers.Dropout(0.5),  # Regularización por abandono\n",
        "    layers.Dense(10, activation='softmax')  # Capa de salida con activación Softmax para clasificación multiclase\n",
        "])\n",
        "\n",
        "# Compilar el modelo con diferentes optimizadores y la función de pérdida de entropía cruzada categórica\n",
        "optimizers = [\n",
        "    tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    RectifiedAdam(learning_rate=learning_rate),\n",
        "    tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=True)  # AMSGrad\n",
        "]\n",
        "\n",
        "# Entrenar el modelo con diferentes optimizadores y funciones de activación\n",
        "for optimizer in optimizers:\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de prueba\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "    print(f\"\\nOptimizer: {optimizer.get_config()['name']}\")\n",
        "    print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "    # Graficar la pérdida y la precisión durante el entrenamiento\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Graficar la pérdida\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Graficar la precisión\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "532IvXiAItCs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}